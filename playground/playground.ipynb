{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# prevent warnings of bert from showing up\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "fake = pd.read_csv(\"../data/Fake.csv\")\n",
    "fake[\"label\"] = 0\n",
    "\n",
    "true = pd.read_csv(\"../data/True.csv\")\n",
    "true[\"label\"] = 1\n",
    "\n",
    "data = pd.concat([fake, true], ignore_index=True)\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"X\"] = train[\"author\"] + \"[SEP]\" + train[\"title\"] + \"[SEP]\" + train[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove row if a value is nan\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"X\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>Paul Ryan Exposed As SPINELESS As Leaked Audi...</td>\n",
       "      <td>It s no secret that Speaker of the House Paul ...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 14, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>News[SEP] Paul Ryan Exposed As SPINELESS As Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>‘CLOWN SHOW’: Former CIA Official Takes Shots...</td>\n",
       "      <td>Philip Mudd, a former CIA official, totally to...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 6, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>News[SEP] ‘CLOWN SHOW’: Former CIA Official Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11755</th>\n",
       "      <td>#Berkeley IRONY ALERT! ANARCHISTS LOOT STARBUC...</td>\n",
       "      <td>Smashing windows How progressive!Protests aga...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>politics[SEP]#Berkeley IRONY ALERT! ANARCHISTS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33950</th>\n",
       "      <td>Pennsylvania governor raises minimum wage for ...</td>\n",
       "      <td>HARRISBURG, Pa. (Reuters) - Pennsylvania Gover...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 7, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>politicsNews[SEP]Pennsylvania governor raises ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>LEAKED AUDIO: MSNBC Worked With Trump During ...</td>\n",
       "      <td>For those of us who watched the shameful MSNBC...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 22, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>News[SEP] LEAKED AUDIO: MSNBC Worked With Trum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "2151    Paul Ryan Exposed As SPINELESS As Leaked Audi...   \n",
       "3533    ‘CLOWN SHOW’: Former CIA Official Takes Shots...   \n",
       "11755  #Berkeley IRONY ALERT! ANARCHISTS LOOT STARBUC...   \n",
       "33950  Pennsylvania governor raises minimum wage for ...   \n",
       "7927    LEAKED AUDIO: MSNBC Worked With Trump During ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "2151   It s no secret that Speaker of the House Paul ...          News   \n",
       "3533   Philip Mudd, a former CIA official, totally to...          News   \n",
       "11755   Smashing windows How progressive!Protests aga...      politics   \n",
       "33950  HARRISBURG, Pa. (Reuters) - Pennsylvania Gover...  politicsNews   \n",
       "7927   For those of us who watched the shameful MSNBC...          News   \n",
       "\n",
       "                    date  label  \\\n",
       "2151      March 14, 2017      0   \n",
       "3533    December 6, 2016      0   \n",
       "11755        Feb 2, 2017      0   \n",
       "33950     March 7, 2016       1   \n",
       "7927   February 22, 2016      0   \n",
       "\n",
       "                                                       X  \n",
       "2151   News[SEP] Paul Ryan Exposed As SPINELESS As Le...  \n",
       "3533   News[SEP] ‘CLOWN SHOW’: Former CIA Official Ta...  \n",
       "11755  politics[SEP]#Berkeley IRONY ALERT! ANARCHISTS...  \n",
       "33950  politicsNews[SEP]Pennsylvania governor raises ...  \n",
       "7927   News[SEP] LEAKED AUDIO: MSNBC Worked With Trum...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"X\"] = data[\"subject\"] + \"[SEP]\" + data[\"title\"] + \"[SEP]\" + data[\"text\"]\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politicsNews       11272\n",
       "worldnews          10145\n",
       "News                9050\n",
       "politics            6841\n",
       "left-news           4459\n",
       "Government News     1570\n",
       "US_News              783\n",
       "Middle-east          778\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"subject\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "\n",
    "# Get pytorch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model   \n",
    "\n",
    "class CustomBERTModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model, BERT_MODEL = \"bert-base-uncased\"):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "\n",
    "        self.bert_model = BertModel.from_pretrained(BERT_MODEL, output_hidden_states=True)\n",
    "\n",
    "        # set a linear layer to map the hidden states to 64 dimensions\n",
    "        self.linear = torch.nn.Linear(768, 64)\n",
    "        # set another linear layer to map the 64 dimensions to 2 dimensions\n",
    "        self.linear2 = torch.nn.Linear(64, 2)\n",
    "        # set a dropout layer\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        # set a relu activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert_model(input_ids)\n",
    "\n",
    "        # pass the last hidden state of the token `[CLS]` to the linear layer\n",
    "        x = self.linear(outputs[0][:,0,:])\n",
    "\n",
    "        # pass the output of the linear layer to the relu activation function\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # pass the output of the relu activation function to the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # pass the output of the dropout layer to the second linear layer\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # set a softmax activation function\n",
    "        x = torch.nn.functional.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "  Average training loss: 0.68\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.61\n",
      "  Training epcoh took: 0:00:28\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m sentences \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[:\u001b[39m1000\u001b[39m]\n\u001b[1;32m    138\u001b[0m labels \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[:\u001b[39m1000\u001b[39m]\n\u001b[0;32m--> 139\u001b[0m train(sentences, labels)\n",
      "Cell \u001b[0;32mIn[3], line 99\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(sentences, labels, lower)\u001b[0m\n\u001b[1;32m     97\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     98\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 99\u001b[0m     model\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m    101\u001b[0m avg_train_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m    102\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Average training loss: \u001b[39m\u001b[39m{0:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(avg_train_loss))\n",
      "File \u001b[0;32m~/Downloads/fake-news/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1971\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_replica\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1965\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1966\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCalling .zero_grad() from a module created with nn.DataParallel() has no effect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe parameters are copied (in a differentiable manner) from the original module. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1968\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis means they are not leaf nodes in autograd and so don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt accumulate gradients. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you need gradients in your forward method, consider using autograd.grad instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1971\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m   1972\u001b[0m     \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1973\u001b[0m         \u001b[39mif\u001b[39;00m set_to_none:\n",
      "File \u001b[0;32m~/Downloads/fake-news/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1710\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparameters\u001b[39m(\u001b[39mself\u001b[39m, recurse: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   1689\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \n\u001b[1;32m   1691\u001b[0m \u001b[39m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \n\u001b[1;32m   1709\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1710\u001b[0m     \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters(recurse\u001b[39m=\u001b[39mrecurse):\n\u001b[1;32m   1711\u001b[0m         \u001b[39myield\u001b[39;00m param\n",
      "File \u001b[0;32m~/Downloads/fake-news/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1737\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[39mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1732\u001b[0m \n\u001b[1;32m   1733\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[1;32m   1735\u001b[0m     \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39m_parameters\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1736\u001b[0m     prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse)\n\u001b[0;32m-> 1737\u001b[0m \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m gen:\n\u001b[1;32m   1738\u001b[0m     \u001b[39myield\u001b[39;00m elem\n",
      "File \u001b[0;32m~/Downloads/fake-news/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1679\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1677\u001b[0m memo \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   1678\u001b[0m modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules(prefix\u001b[39m=\u001b[39mprefix) \u001b[39mif\u001b[39;00m recurse \u001b[39melse\u001b[39;00m [(prefix, \u001b[39mself\u001b[39m)]\n\u001b[0;32m-> 1679\u001b[0m \u001b[39mfor\u001b[39;00m module_prefix, module \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1680\u001b[0m     members \u001b[39m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   1681\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m members:\n",
      "File \u001b[0;32m~/Downloads/fake-news/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1890\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1888\u001b[0m submodule_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m prefix \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name\n\u001b[1;32m   1889\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[0;32m-> 1890\u001b[0m     \u001b[39myield\u001b[39;00m m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define flat_accuracy function\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# define format_time function\n",
    "def format_time(elapsed):\n",
    "    import datetime\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def train(sentences, labels, lower = False):\n",
    "\n",
    "    # set epochs\n",
    "    EPOCHS = 10\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "\n",
    "    # define bert tokenizer\n",
    "    tokenizer = BertTokenizer.# Get pytorch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "e_plus(\n",
    "                sentences[i],\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=64,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "        input_ids.append(inputs[\"input_ids\"].squeeze(0))\n",
    "\n",
    "        # convert labels to one-hot encoding\n",
    "        target = torch.zeros(2)\n",
    "        target[labels[i]] = 1\n",
    "        targets.append(target)\n",
    "    \n",
    "    # convert to tensors\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    # create dataset\n",
    "    dataset = TensorDataset(input_ids, targets)\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "    # define model\n",
    "    model = CustomBERTModel(BertModel, BERT_MODEL = \"bert-base-uncased\").to(device)\n",
    "\n",
    "    # set adamw optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    # set loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # set number of training steps\n",
    "    total_steps = len(dataloader) * EPOCHS\n",
    "    # set scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-5, steps_per_epoch=len(dataloader), epochs=EPOCHS)\n",
    "\n",
    "    # start training clock\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), f\"../models/bert_{epoch}.pt\")\n",
    "\n",
    "        # # evaluate model\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "        avg_val_accuracy = total_eval_accuracy / len(dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(dataloader)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - start_time)))\n",
    "\n",
    "sentences = data[\"title\"].values[:1000]\n",
    "labels = data[\"label\"].values[:1000]\n",
    "train(sentences, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# define predict function\n",
    "def predict(sentence, lower = False):\n",
    "    # define bert tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    if lower:\n",
    "        sentence = sentence.lower()\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # define model\n",
    "    model = CustomBERTModel(BertModel, BERT_MODEL = \"bert-base-uncased\").to(device)\n",
    "\n",
    "    # load model\n",
    "    model.load_state_dict(torch.load(\"../models/bert_9.pt\"))\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(input_ids)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return np.argmax(logits, axis=1)\n",
    "\n",
    "# predict\n",
    "predict(data[\"title\"][0], lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22f22d8c7ef79205120b15562680aba7ac95c4d60484d46c588434809bc3bffd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
